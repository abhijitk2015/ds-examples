{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Autoencoders for Anomaly Detection\n",
    "\n",
    "Dataset: \n",
    "https://www.kaggle.com/jboysen/spy-plane-finder\n",
    "\n",
    "This lab will focus on using an autoencoder for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import dataset\n",
    "df=pd.read_csv(\"data/planes_features.csv\")\n",
    "\n",
    "#convert type column to integer (there are probably better approaches to use here)\n",
    "df['type']=df['type'].astype('category').cat.codes\n",
    "\n",
    "#import labelled aircraft\n",
    "test_ident = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "#use the labelled data as a train/test set (note the different context of testing to normal for this model)\n",
    "labelled_data=df[df['adshex'].isin(test_ident['adshex'])]\n",
    "labelled_data=pd.merge(labelled_data,test_ident,on=['adshex','adshex'])\n",
    "labelled_data['type']=labelled_data['type'].astype('category').cat.codes\n",
    "labelled_data=labelled_data.drop(['adshex'],axis=1)\n",
    "\n",
    "df=df[~df['adshex'].isin(test_ident['adshex'])]\n",
    "df_adshex=df['adshex']\n",
    "df=df.drop(['adshex'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration1</th>\n",
       "      <th>duration2</th>\n",
       "      <th>duration3</th>\n",
       "      <th>duration4</th>\n",
       "      <th>duration5</th>\n",
       "      <th>boxes1</th>\n",
       "      <th>boxes2</th>\n",
       "      <th>boxes3</th>\n",
       "      <th>boxes4</th>\n",
       "      <th>boxes5</th>\n",
       "      <th>...</th>\n",
       "      <th>steer3</th>\n",
       "      <th>steer4</th>\n",
       "      <th>steer5</th>\n",
       "      <th>steer6</th>\n",
       "      <th>steer7</th>\n",
       "      <th>steer8</th>\n",
       "      <th>flights</th>\n",
       "      <th>squawk_1</th>\n",
       "      <th>observations</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.075949</td>\n",
       "      <td>0.183544</td>\n",
       "      <td>0.335443</td>\n",
       "      <td>0.284810</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>0.069620</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270550</td>\n",
       "      <td>0.344090</td>\n",
       "      <td>0.097317</td>\n",
       "      <td>0.186651</td>\n",
       "      <td>0.011379</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>11776</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211735</td>\n",
       "      <td>0.155612</td>\n",
       "      <td>0.181122</td>\n",
       "      <td>0.198980</td>\n",
       "      <td>0.252551</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.168367</td>\n",
       "      <td>0.173469</td>\n",
       "      <td>0.267857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.356314</td>\n",
       "      <td>0.116116</td>\n",
       "      <td>0.159325</td>\n",
       "      <td>0.012828</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>392</td>\n",
       "      <td>0</td>\n",
       "      <td>52465</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202665</td>\n",
       "      <td>0.380515</td>\n",
       "      <td>0.094669</td>\n",
       "      <td>0.182904</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2176</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249206</td>\n",
       "      <td>0.326984</td>\n",
       "      <td>0.112698</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>10</td>\n",
       "      <td>1135</td>\n",
       "      <td>630</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281646</td>\n",
       "      <td>0.416139</td>\n",
       "      <td>0.112342</td>\n",
       "      <td>0.169304</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>6</td>\n",
       "      <td>2356</td>\n",
       "      <td>632</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration1  duration2  duration3  duration4  duration5    boxes1    boxes2  \\\n",
       "0   0.120253   0.075949   0.183544   0.335443   0.284810  0.088608  0.044304   \n",
       "1   0.211735   0.155612   0.181122   0.198980   0.252551  0.204082  0.183673   \n",
       "3   0.125000   0.041667   0.208333   0.166667   0.458333  0.125000  0.083333   \n",
       "4   0.100000   0.200000   0.200000   0.400000   0.100000  0.100000  0.000000   \n",
       "5   0.166667   0.166667   0.000000   0.666667   0.000000  0.333333  0.000000   \n",
       "\n",
       "     boxes3    boxes4    boxes5  ...     steer3    steer4    steer5    steer6  \\\n",
       "0  0.069620  0.120253  0.677215  ...   0.270550  0.344090  0.097317  0.186651   \n",
       "1  0.168367  0.173469  0.267857  ...   0.240732  0.356314  0.116116  0.159325   \n",
       "3  0.125000  0.166667  0.500000  ...   0.202665  0.380515  0.094669  0.182904   \n",
       "4  0.100000  0.400000  0.400000  ...   0.249206  0.326984  0.112698  0.206349   \n",
       "5  0.000000  0.666667  0.000000  ...   0.281646  0.416139  0.112342  0.169304   \n",
       "\n",
       "     steer7    steer8  flights  squawk_1  observations  type  \n",
       "0  0.011379  0.009426      158         0         11776   248  \n",
       "1  0.012828  0.013628      392         0         52465   431  \n",
       "3  0.014706  0.020221       24         0          2176   350  \n",
       "4  0.012698  0.011111       10      1135           630   126  \n",
       "5  0.001582  0.001582        6      2356           632   133  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% of the aircraft from the main dataset are then removed and used to build a training set. Note, the data was scaled between 0-1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 10% of the input data as a training set\n",
    "df,train_set=train_test_split(df,test_size=0.1,random_state=57)\n",
    "\n",
    "#also consider adding some of the actual data to the train/test set to improve the size\n",
    "test_set = labelled_data\n",
    "\n",
    "#save test set labels for later\n",
    "test_set_labels=test_set['class']\n",
    "#get number of positive classes in test set\n",
    "test_set_positives=len(test_set[test_set['class']=='surveil'])\n",
    "test_set = test_set.drop(['class'], axis=1)\n",
    "\n",
    "#convert to array and normalise\n",
    "train_set = preprocessing.MinMaxScaler().fit_transform(train_set.values)\n",
    "test_set = preprocessing.MinMaxScaler().fit_transform(test_set.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim was to find an algorithm which would identify outliers in the data. For this dataset, aircraft which behave oddly may well be surveillance aircraft. I used an autoencoder neural network to do this, here's how they work:\n",
    "\n",
    "I'm not going to go into too much detail about autoencoders. In a sentance, they take the input data, compress it, and then try and 're-predict' the input data from the compressed version. Here's how you'd represent one, with nodes and links:\n",
    "\n",
    "![alt text](autoencoder_image.png \"Title\")\n",
    "\n",
    "in the example above, a noisy image forms the input data. By compressing it and then recreating it, the noise is removed, giving a clearer version of the original image. \n",
    "At first glance this seems weird: training something to recreate the input data? This becomes useful when we consider what happens if we feed the network an unusual data point. By training it on typical data points, it learns to approximat to something which is typical. However, if the trained network is then given an anomolous point, it is likely to recreate it with a high degree of error. By measuring the difference (ie error) between the input data and the recreated points, we can identify which are the largest outliers in the data.\n",
    "\n",
    "In other words, a properly trained autoencoder can spot unusual data points.\n",
    "\n",
    "To do this with Keras/Tensorflow we first define the layers of the network. The input and output layers must be the same size as the data, with a node for each attribute. The intermediate layers wre half the size, to allow the network to compress the records as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define layers\n",
    "input_dim = test_set.shape[1]\n",
    "encoding_dim = int(input_dim/2)\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the model was run and the best performing model saved. Note, all the parameters like the optimiser and the loss function. These can be changes along with the number and size of the layers. This process is called hyperparameter tuning and we seek to find the best combination of hyperparameters to obtain the best results. \n",
    "\n",
    "Note: the model was only run on the 10% of data which were used as the training set. We would usually like to train only on data that is of the negative (normal) class, but in this case a few positive examples shouldn't make any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1921 samples, validate on 597 samples\n",
      "Epoch 1/100\n",
      "1921/1921 [==============================] - 1s 373us/step - loss: 0.0873 - acc: 0.0578 - val_loss: 0.0793 - val_acc: 0.0436\n",
      "Epoch 2/100\n",
      "1921/1921 [==============================] - 0s 107us/step - loss: 0.0676 - acc: 0.0817 - val_loss: 0.0704 - val_acc: 0.0536\n",
      "Epoch 3/100\n",
      "1921/1921 [==============================] - 0s 134us/step - loss: 0.0612 - acc: 0.0984 - val_loss: 0.0658 - val_acc: 0.0586\n",
      "Epoch 4/100\n",
      "1921/1921 [==============================] - 0s 124us/step - loss: 0.0576 - acc: 0.1197 - val_loss: 0.0630 - val_acc: 0.0670\n",
      "Epoch 5/100\n",
      "1921/1921 [==============================] - 0s 135us/step - loss: 0.0550 - acc: 0.1577 - val_loss: 0.0608 - val_acc: 0.0603\n",
      "Epoch 6/100\n",
      "1921/1921 [==============================] - 0s 145us/step - loss: 0.0530 - acc: 0.1671 - val_loss: 0.0591 - val_acc: 0.0553\n",
      "Epoch 7/100\n",
      "1921/1921 [==============================] - 0s 114us/step - loss: 0.0513 - acc: 0.1775 - val_loss: 0.0575 - val_acc: 0.0603\n",
      "Epoch 8/100\n",
      "1921/1921 [==============================] - 0s 87us/step - loss: 0.0499 - acc: 0.1957 - val_loss: 0.0562 - val_acc: 0.0653\n",
      "Epoch 9/100\n",
      "1921/1921 [==============================] - 0s 96us/step - loss: 0.0486 - acc: 0.2145 - val_loss: 0.0549 - val_acc: 0.0804\n",
      "Epoch 10/100\n",
      "1921/1921 [==============================] - 0s 142us/step - loss: 0.0473 - acc: 0.2348 - val_loss: 0.0537 - val_acc: 0.1055\n",
      "Epoch 11/100\n",
      "1921/1921 [==============================] - 0s 99us/step - loss: 0.0460 - acc: 0.2525 - val_loss: 0.0522 - val_acc: 0.1139\n",
      "Epoch 12/100\n",
      "1921/1921 [==============================] - 0s 125us/step - loss: 0.0447 - acc: 0.2639 - val_loss: 0.0510 - val_acc: 0.1290\n",
      "Epoch 13/100\n",
      "1921/1921 [==============================] - 0s 186us/step - loss: 0.0438 - acc: 0.2697 - val_loss: 0.0500 - val_acc: 0.1374\n",
      "Epoch 14/100\n",
      "1921/1921 [==============================] - 0s 168us/step - loss: 0.0430 - acc: 0.2681 - val_loss: 0.0492 - val_acc: 0.1457\n",
      "Epoch 15/100\n",
      "1921/1921 [==============================] - 0s 174us/step - loss: 0.0423 - acc: 0.2723 - val_loss: 0.0483 - val_acc: 0.1508\n",
      "Epoch 16/100\n",
      "1921/1921 [==============================] - 0s 92us/step - loss: 0.0414 - acc: 0.2775 - val_loss: 0.0475 - val_acc: 0.1541\n",
      "Epoch 17/100\n",
      "1921/1921 [==============================] - 0s 124us/step - loss: 0.0406 - acc: 0.2806 - val_loss: 0.0465 - val_acc: 0.1558\n",
      "Epoch 18/100\n",
      "1921/1921 [==============================] - 0s 155us/step - loss: 0.0397 - acc: 0.2863 - val_loss: 0.0456 - val_acc: 0.1524\n",
      "Epoch 19/100\n",
      "1921/1921 [==============================] - 0s 142us/step - loss: 0.0390 - acc: 0.2884 - val_loss: 0.0448 - val_acc: 0.1558\n",
      "Epoch 20/100\n",
      "1921/1921 [==============================] - 0s 73us/step - loss: 0.0383 - acc: 0.2905 - val_loss: 0.0442 - val_acc: 0.1591\n",
      "Epoch 21/100\n",
      "1921/1921 [==============================] - 0s 111us/step - loss: 0.0375 - acc: 0.2967 - val_loss: 0.0426 - val_acc: 0.1675\n",
      "Epoch 22/100\n",
      "1921/1921 [==============================] - 0s 110us/step - loss: 0.0342 - acc: 0.3045 - val_loss: 0.0370 - val_acc: 0.1859\n",
      "Epoch 23/100\n",
      "1921/1921 [==============================] - 0s 137us/step - loss: 0.0284 - acc: 0.2993 - val_loss: 0.0323 - val_acc: 0.1725\n",
      "Epoch 24/100\n",
      "1921/1921 [==============================] - 0s 133us/step - loss: 0.0253 - acc: 0.3113 - val_loss: 0.0303 - val_acc: 0.1809\n",
      "Epoch 25/100\n",
      "1921/1921 [==============================] - 0s 146us/step - loss: 0.0241 - acc: 0.3342 - val_loss: 0.0291 - val_acc: 0.2178\n",
      "Epoch 26/100\n",
      "1921/1921 [==============================] - 0s 127us/step - loss: 0.0229 - acc: 0.3566 - val_loss: 0.0278 - val_acc: 0.2462\n",
      "Epoch 27/100\n",
      "1921/1921 [==============================] - 0s 118us/step - loss: 0.0218 - acc: 0.3748 - val_loss: 0.0272 - val_acc: 0.2496\n",
      "Epoch 28/100\n",
      "1921/1921 [==============================] - 0s 135us/step - loss: 0.0212 - acc: 0.3800 - val_loss: 0.0265 - val_acc: 0.2714\n",
      "Epoch 29/100\n",
      "1921/1921 [==============================] - 0s 161us/step - loss: 0.0208 - acc: 0.3873 - val_loss: 0.0261 - val_acc: 0.2680\n",
      "Epoch 30/100\n",
      "1921/1921 [==============================] - 0s 116us/step - loss: 0.0203 - acc: 0.3925 - val_loss: 0.0258 - val_acc: 0.2647\n",
      "Epoch 31/100\n",
      "1921/1921 [==============================] - 0s 176us/step - loss: 0.0199 - acc: 0.4019 - val_loss: 0.0253 - val_acc: 0.2915\n",
      "Epoch 32/100\n",
      "1921/1921 [==============================] - 0s 156us/step - loss: 0.0196 - acc: 0.4112 - val_loss: 0.0249 - val_acc: 0.2965\n",
      "Epoch 33/100\n",
      "1921/1921 [==============================] - 0s 151us/step - loss: 0.0192 - acc: 0.4217 - val_loss: 0.0245 - val_acc: 0.3082\n",
      "Epoch 34/100\n",
      "1921/1921 [==============================] - 0s 147us/step - loss: 0.0189 - acc: 0.4274 - val_loss: 0.0245 - val_acc: 0.2948\n",
      "Epoch 35/100\n",
      "1921/1921 [==============================] - 0s 113us/step - loss: 0.0185 - acc: 0.4394 - val_loss: 0.0239 - val_acc: 0.3233\n",
      "Epoch 36/100\n",
      "1921/1921 [==============================] - 0s 125us/step - loss: 0.0182 - acc: 0.4482 - val_loss: 0.0235 - val_acc: 0.3333\n",
      "Epoch 37/100\n",
      "1921/1921 [==============================] - 0s 118us/step - loss: 0.0179 - acc: 0.4623 - val_loss: 0.0233 - val_acc: 0.3434\n",
      "Epoch 38/100\n",
      "1921/1921 [==============================] - 0s 142us/step - loss: 0.0176 - acc: 0.4716 - val_loss: 0.0230 - val_acc: 0.3668\n",
      "Epoch 39/100\n",
      "1921/1921 [==============================] - 0s 142us/step - loss: 0.0174 - acc: 0.4904 - val_loss: 0.0227 - val_acc: 0.4070\n",
      "Epoch 40/100\n",
      "1921/1921 [==============================] - 0s 119us/step - loss: 0.0171 - acc: 0.4997 - val_loss: 0.0227 - val_acc: 0.3903\n",
      "Epoch 41/100\n",
      "1921/1921 [==============================] - 0s 140us/step - loss: 0.0169 - acc: 0.5107 - val_loss: 0.0223 - val_acc: 0.4087\n",
      "Epoch 42/100\n",
      "1921/1921 [==============================] - 0s 157us/step - loss: 0.0167 - acc: 0.5148 - val_loss: 0.0221 - val_acc: 0.4204\n",
      "Epoch 43/100\n",
      "1921/1921 [==============================] - 0s 124us/step - loss: 0.0165 - acc: 0.5221 - val_loss: 0.0218 - val_acc: 0.4322\n",
      "Epoch 44/100\n",
      "1921/1921 [==============================] - 0s 122us/step - loss: 0.0163 - acc: 0.5263 - val_loss: 0.0218 - val_acc: 0.4238\n",
      "Epoch 45/100\n",
      "1921/1921 [==============================] - 0s 124us/step - loss: 0.0161 - acc: 0.5429 - val_loss: 0.0215 - val_acc: 0.4506\n",
      "Epoch 46/100\n",
      "1921/1921 [==============================] - 0s 120us/step - loss: 0.0160 - acc: 0.5471 - val_loss: 0.0214 - val_acc: 0.4573\n",
      "Epoch 47/100\n",
      "1921/1921 [==============================] - 0s 123us/step - loss: 0.0158 - acc: 0.5580 - val_loss: 0.0214 - val_acc: 0.4673\n",
      "Epoch 48/100\n",
      "1921/1921 [==============================] - 0s 150us/step - loss: 0.0157 - acc: 0.5711 - val_loss: 0.0211 - val_acc: 0.4941\n",
      "Epoch 49/100\n",
      "1921/1921 [==============================] - 0s 120us/step - loss: 0.0156 - acc: 0.5747 - val_loss: 0.0215 - val_acc: 0.4640\n",
      "Epoch 50/100\n",
      "1921/1921 [==============================] - 0s 108us/step - loss: 0.0154 - acc: 0.5820 - val_loss: 0.0211 - val_acc: 0.5025\n",
      "Epoch 51/100\n",
      "1921/1921 [==============================] - 0s 150us/step - loss: 0.0153 - acc: 0.5825 - val_loss: 0.0213 - val_acc: 0.4941\n",
      "Epoch 52/100\n",
      "1921/1921 [==============================] - 0s 117us/step - loss: 0.0151 - acc: 0.5976 - val_loss: 0.0203 - val_acc: 0.5243\n",
      "Epoch 53/100\n",
      "1921/1921 [==============================] - 0s 147us/step - loss: 0.0148 - acc: 0.5914 - val_loss: 0.0204 - val_acc: 0.5176\n",
      "Epoch 54/100\n",
      "1921/1921 [==============================] - 0s 103us/step - loss: 0.0146 - acc: 0.5903 - val_loss: 0.0199 - val_acc: 0.5126\n",
      "Epoch 55/100\n",
      "1921/1921 [==============================] - 0s 143us/step - loss: 0.0143 - acc: 0.5934 - val_loss: 0.0196 - val_acc: 0.5226\n",
      "Epoch 56/100\n",
      "1921/1921 [==============================] - 0s 125us/step - loss: 0.0141 - acc: 0.5924 - val_loss: 0.0196 - val_acc: 0.5193\n",
      "Epoch 57/100\n",
      "1921/1921 [==============================] - 0s 77us/step - loss: 0.0139 - acc: 0.5997 - val_loss: 0.0194 - val_acc: 0.5159\n",
      "Epoch 58/100\n",
      "1921/1921 [==============================] - 0s 194us/step - loss: 0.0138 - acc: 0.6012 - val_loss: 0.0191 - val_acc: 0.5126\n",
      "Epoch 59/100\n",
      "1921/1921 [==============================] - 0s 127us/step - loss: 0.0136 - acc: 0.6028 - val_loss: 0.0190 - val_acc: 0.5092\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1921/1921 [==============================] - 0s 162us/step - loss: 0.0135 - acc: 0.6023 - val_loss: 0.0188 - val_acc: 0.5159\n",
      "Epoch 61/100\n",
      "1921/1921 [==============================] - 0s 138us/step - loss: 0.0134 - acc: 0.6028 - val_loss: 0.0190 - val_acc: 0.5025\n",
      "Epoch 62/100\n",
      "1921/1921 [==============================] - 0s 106us/step - loss: 0.0133 - acc: 0.6070 - val_loss: 0.0188 - val_acc: 0.5042\n",
      "Epoch 63/100\n",
      "1921/1921 [==============================] - 0s 126us/step - loss: 0.0132 - acc: 0.6122 - val_loss: 0.0188 - val_acc: 0.5075\n",
      "Epoch 64/100\n",
      "1921/1921 [==============================] - 0s 139us/step - loss: 0.0131 - acc: 0.6111 - val_loss: 0.0187 - val_acc: 0.5008\n",
      "Epoch 65/100\n",
      "1921/1921 [==============================] - 0s 122us/step - loss: 0.0130 - acc: 0.6143 - val_loss: 0.0189 - val_acc: 0.4975\n",
      "Epoch 66/100\n",
      "1921/1921 [==============================] - 0s 118us/step - loss: 0.0130 - acc: 0.6054 - val_loss: 0.0187 - val_acc: 0.5109\n",
      "Epoch 67/100\n",
      "1921/1921 [==============================] - 0s 92us/step - loss: 0.0129 - acc: 0.6143 - val_loss: 0.0186 - val_acc: 0.5042\n",
      "Epoch 68/100\n",
      "1921/1921 [==============================] - 0s 110us/step - loss: 0.0129 - acc: 0.6137 - val_loss: 0.0188 - val_acc: 0.5059\n",
      "Epoch 69/100\n",
      "1921/1921 [==============================] - 0s 97us/step - loss: 0.0128 - acc: 0.6091 - val_loss: 0.0185 - val_acc: 0.5008\n",
      "Epoch 70/100\n",
      "1921/1921 [==============================] - 0s 165us/step - loss: 0.0128 - acc: 0.6117 - val_loss: 0.0183 - val_acc: 0.5159\n",
      "Epoch 71/100\n",
      "1921/1921 [==============================] - 0s 111us/step - loss: 0.0127 - acc: 0.6117 - val_loss: 0.0183 - val_acc: 0.5075\n",
      "Epoch 72/100\n",
      "1921/1921 [==============================] - 0s 125us/step - loss: 0.0127 - acc: 0.6158 - val_loss: 0.0185 - val_acc: 0.5042\n",
      "Epoch 73/100\n",
      "1921/1921 [==============================] - 0s 67us/step - loss: 0.0126 - acc: 0.6169 - val_loss: 0.0184 - val_acc: 0.5109\n",
      "Epoch 74/100\n",
      "1921/1921 [==============================] - 0s 94us/step - loss: 0.0126 - acc: 0.6174 - val_loss: 0.0188 - val_acc: 0.4841\n",
      "Epoch 75/100\n",
      "1921/1921 [==============================] - 0s 176us/step - loss: 0.0126 - acc: 0.6143 - val_loss: 0.0184 - val_acc: 0.4925\n",
      "Epoch 76/100\n",
      "1921/1921 [==============================] - 0s 108us/step - loss: 0.0126 - acc: 0.6158 - val_loss: 0.0181 - val_acc: 0.5142\n",
      "Epoch 77/100\n",
      "1921/1921 [==============================] - 0s 77us/step - loss: 0.0125 - acc: 0.6210 - val_loss: 0.0181 - val_acc: 0.5092\n",
      "Epoch 78/100\n",
      "1921/1921 [==============================] - 0s 151us/step - loss: 0.0125 - acc: 0.6216 - val_loss: 0.0181 - val_acc: 0.4992\n",
      "Epoch 79/100\n",
      "1921/1921 [==============================] - 0s 152us/step - loss: 0.0125 - acc: 0.6216 - val_loss: 0.0180 - val_acc: 0.5159\n",
      "Epoch 80/100\n",
      "1921/1921 [==============================] - 0s 134us/step - loss: 0.0124 - acc: 0.6169 - val_loss: 0.0183 - val_acc: 0.4941\n",
      "Epoch 81/100\n",
      "1921/1921 [==============================] - 0s 147us/step - loss: 0.0124 - acc: 0.6195 - val_loss: 0.0180 - val_acc: 0.5092\n",
      "Epoch 82/100\n",
      "1921/1921 [==============================] - 0s 156us/step - loss: 0.0124 - acc: 0.6205 - val_loss: 0.0180 - val_acc: 0.4925\n",
      "Epoch 83/100\n",
      "1921/1921 [==============================] - 0s 188us/step - loss: 0.0123 - acc: 0.6179 - val_loss: 0.0180 - val_acc: 0.4975\n",
      "Epoch 84/100\n",
      "1921/1921 [==============================] - 0s 147us/step - loss: 0.0123 - acc: 0.6257 - val_loss: 0.0181 - val_acc: 0.4975\n",
      "Epoch 85/100\n",
      "1921/1921 [==============================] - 0s 137us/step - loss: 0.0123 - acc: 0.6205 - val_loss: 0.0180 - val_acc: 0.4925\n",
      "Epoch 86/100\n",
      "1921/1921 [==============================] - 0s 154us/step - loss: 0.0123 - acc: 0.6184 - val_loss: 0.0177 - val_acc: 0.5109\n",
      "Epoch 87/100\n",
      "1921/1921 [==============================] - 0s 150us/step - loss: 0.0122 - acc: 0.6242 - val_loss: 0.0178 - val_acc: 0.5109\n",
      "Epoch 88/100\n",
      "1921/1921 [==============================] - 0s 208us/step - loss: 0.0122 - acc: 0.6226 - val_loss: 0.0178 - val_acc: 0.5042\n",
      "Epoch 89/100\n",
      "1921/1921 [==============================] - 0s 140us/step - loss: 0.0122 - acc: 0.6195 - val_loss: 0.0177 - val_acc: 0.5075\n",
      "Epoch 90/100\n",
      "1921/1921 [==============================] - 0s 117us/step - loss: 0.0122 - acc: 0.6210 - val_loss: 0.0178 - val_acc: 0.5042\n",
      "Epoch 91/100\n",
      "1921/1921 [==============================] - 0s 137us/step - loss: 0.0121 - acc: 0.6257 - val_loss: 0.0175 - val_acc: 0.5075\n",
      "Epoch 92/100\n",
      "1921/1921 [==============================] - 0s 161us/step - loss: 0.0121 - acc: 0.6257 - val_loss: 0.0172 - val_acc: 0.5142\n",
      "Epoch 93/100\n",
      "1921/1921 [==============================] - 0s 166us/step - loss: 0.0120 - acc: 0.6273 - val_loss: 0.0176 - val_acc: 0.4925\n",
      "Epoch 94/100\n",
      "1921/1921 [==============================] - 0s 163us/step - loss: 0.0120 - acc: 0.6278 - val_loss: 0.0173 - val_acc: 0.5008\n",
      "Epoch 95/100\n",
      "1921/1921 [==============================] - 0s 113us/step - loss: 0.0120 - acc: 0.6247 - val_loss: 0.0174 - val_acc: 0.4975\n",
      "Epoch 96/100\n",
      "1921/1921 [==============================] - 0s 159us/step - loss: 0.0119 - acc: 0.6247 - val_loss: 0.0174 - val_acc: 0.5025\n",
      "Epoch 97/100\n",
      "1921/1921 [==============================] - 0s 141us/step - loss: 0.0119 - acc: 0.6268 - val_loss: 0.0174 - val_acc: 0.5025\n",
      "Epoch 98/100\n",
      "1921/1921 [==============================] - 0s 185us/step - loss: 0.0118 - acc: 0.6273 - val_loss: 0.0170 - val_acc: 0.5092\n",
      "Epoch 99/100\n",
      "1921/1921 [==============================] - 0s 191us/step - loss: 0.0118 - acc: 0.6304 - val_loss: 0.0168 - val_acc: 0.5159\n",
      "Epoch 100/100\n",
      "1921/1921 [==============================] - 0s 142us/step - loss: 0.0118 - acc: 0.6314 - val_loss: 0.0172 - val_acc: 0.4925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_epoch = 100\n",
    "batch_size = 50\n",
    "autoencoder.compile(optimizer='Adamax', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(train_set,train_set,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test_set,test_set),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history\n",
    "\n",
    "autoencoder=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to test the model on the dataset of 'known' aircraft. To classify aircraft,the model only labelled the top 97 anomalous entries as spyplanes, because that is how many were labelled as such in the original 'known' aircraft dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[451  49]\n",
      " [ 49  48]]\n"
     ]
    }
   ],
   "source": [
    "#predict on testing set\n",
    "predictions=autoencoder.predict(test_set)\n",
    "rmse = pow(np.mean(np.power(test_set - predictions, 2), axis=1),0.5)\n",
    "error_table = pd.DataFrame({'reconstruction_error': rmse,\n",
    "                        'actual_class': test_set_labels})\n",
    "#we know how many entries have a positive in the test set. Take this number and label the predictions with the highest rmse (ie the outliers) with the positiveclass prediction\n",
    "error_table['predicted_class'] = \n",
    "np.where(error_table['reconstruction_error'] >= \n",
    "         min(error_table.nlargest(int(test_set_positives),'reconstruction_error', keep='first')['reconstruction_error']), 'surveil', 'other')\n",
    "\n",
    "#get confusion matrix\n",
    "print(confusion_matrix(error_table['actual_class'],error_table['predicted_class']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the parameters, we managed to improve this to 54, which gives the following confusion matrix:\n",
    "\n",
    "|              |   normal   |  spyplane     |\n",
    "|--------------|------------|---------------|\n",
    "|   normal     |    457     |       43      |\n",
    "|--------------|------------|---------------|\n",
    "|   spyplane   |     43     |      54       |\n",
    "\n",
    "While this might not appear to have great accuracy, it did find 54 of the 97 aircraft which were known to be government operated. Let's remember that the algorithm is identifying unusual aircraft just by the fact they fly in an irregular pattern - it is not using any known data to correlate observed aircraft with known spyplanes.\n",
    "\n",
    "Let's also remember that some of the aircraft identified as spyplanes which were labelled as 'normal' may indeed be spyplanes. The test data relies on accurate labelling of aircraft which is impossible.\n",
    "\n",
    "Let's have a look at the reconstruction error for each of the aircraft in the test set. This is a measure of how much of an outlier each aircraft was - recall the top 97 outliers were taken to be spyplanes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztvXucVNWV6P9dVdXQaEQDaFSQ0IAE0SRiOkKDwWcbkplJcpWoeUwyxsTrxDxGRhIdc83V3OSXIYp5XGPUBJPcz018T653JhcG37F5BAgYhAB200QaFBQEUaG7q2r//qgHp06fxz5Vp+qcqtrfz4cPXVWn6ux99t5rr732WmuLUgqDwWAwNAeJqAtgMBgMhtphhL7BYDA0EUboGwwGQxNhhL7BYDA0EUboGwwGQxNhhL7BYDA0EUboGwwGQxNhhL7BYDA0EUboGwwGQxORiroAdsaMGaMmTJgQdTEMBoOhrli7du1rSqnj/a6LndCfMGECa9asiboYBoPBUFeIyF91rjPmHYPBYGgijNA3GAyGJsIIfYPBYGgijNA3GAyGJsIIfYPBYGgijNA3GAyGJsIIfYOh2mTS8PgtcO+Fuf8z6ahLZGhiYuenbzA0HE99F1bdBYOHYPdGQOCim6MulaFJMZq+wVBtep/NCXyA9CHofSba8hiaGiP0DYZq0zYHUiNyf6dGQNu50ZbH0NQY847BUG3OvwmQnIbfdi6c/y9Rl8jQxBihbzBUm2TK2PANscGYdwwGg6GJMELfYDAYmggj9A0Gg6GJMELfYDAYmggj9A0Gg6GJMELfYDAYmggtoS8ic0Vki4h0i8gNDp9fIyIbRGS9iDwnItMsn92Y/94WEflwmIU3GAwGQzB8hb6IJIE7gY8A04BPWYV6nt8opd6rlDoTWAgsyn93GnAFcDowF/hp/vcMBoPBEAE6mv7ZQLdSaptSagC4H/i49QKl1BuWl0cDKv/3x4H7lVL9SqleoDv/ewaDwWCIAJ2I3LHADsvrPmCG/SIRuRaYDwwDLrB8d6Xtu2PLKqnBYDAYKkZH0xeH99SQN5S6Uyk1Cfgm8K0g3xWRq0VkjYisefXVVzWKZDAYDIZy0BH6fcApltfjgF0e198PfCLId5VS9yil2pVS7ccff7xGkQwGg8FQDjpCfzVwqoi0icgwchuzj1kvEJFTLS//Bngx//djwBUiMlxE2oBTgT9WXmyDwWAwlIOvTV8plRaRrwBLgSSwWCm1UURuBdYopR4DviIiFwGDwOvA5/Pf3SgiDwKbgDRwrVIqU6W6GAwGg8EHUWqIiT1S2tvb1Zo1a6IuhsFgMNQVIrJWKdXud52JyDUYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCKM0DcYDIYmwgh9g8FgaCJSURfAYDAYGpV0JsuiZVtZ3rOXWZNGM79zCqlktLq2EfoGg8FQJRYt28rirl4OD2bZ/MobCLBg7tRIy2TMOwaDwVAllvfs5fBgFoDDg1m6evZGXCJNoS8ic0Vki4h0i8gNDp/PF5FNIvJnEXlCRN5t+SwjIuvz/x4Ls/AGg8EQZ2ZNGk1rS07MtrYkmD1pdMQl0jDviEgSuBPoBPqA1SLymFJqk+WydUC7UuptEflHYCFwef6zQ0qpM0Mut8FgMMSe+Z1TEKCrZy+zJ43mus4pURdJy6Z/NtCtlNoGICL3Ax8HikJfKfWU5fqVwGfDLKTBYDDUI6lkggVzp7Ig6oJY0DHvjAV2WF735d9z4yrg/1let4rIGhFZKSKfKKOMBoPBYAgJHU1fHN5TjheKfBZoB861vD1eKbVLRCYCT4rIBqVUj+17VwNXA4wfP16r4AaDwWAIjo6m3wecYnk9Dthlv0hELgJuAj6mlOovvK+U2pX/fxvwNDDd/l2l1D1KqXalVPvxxx8fqAIGg8Fg0EdH6K8GThWRNhEZBlwBlHjhiMh04G5yAn+P5f13isjw/N9jgNlY9gIMBoPBUFt8zTtKqbSIfAVYCiSBxUqpjSJyK7BGKfUY8APgHcBDIgLwklLqY8BpwN0ikiU3wXzf5vVjMBgMhhoiSjma5yOjvb1drVmzJupiGAwGQ10hImuVUu1+15k0DA7EMV+GwWAwhIER+g7EMV+GwRBbMml46rvQ+yy0zYHzb4KkES1xxbSMA075MuIUXGGocxpNSD71XVh1Fwwegt0bAYGLbo66VAYX6rinVY9Zk0az+ZU3ODyYjU2+DEMD0WhCsvfZXF0A0oeg95loy2PwxAh9B+KYL8PQQDSakGybk5u80ocgNQLazvX/ThUxe3LeGKHvQBzzZdQzZhDaiJmQrJjzbwIkN3m1nQvn/0ukxTF7ct4YoW+oOmYQ2oiZkKyYZCpW5imzJ+eNEfoGb0LYdKxkEDbkKiFmQrLRMHty3hihb/AmhE3HSgahWSUYgmL25LwxQt/gTQibjpUMQrNUNwTF7Ml5Y4S+wZsQNh0rGYRmqW4whIsR+jWm7mzUEW86mqW6wRAuRujXmLqzUUe86WiW6gZDuMRYxWxMnGzUBoPBUCuM0K8xsyaNprUl99iNjdpgMNQaY96pMcZGbagZjZbYzRAKpgfUmCE26kwaHv+OGZiG8Gm0xG5lUHeOEzXASJeoMQPTUC16n7HFWDwNNFffqjvHiRrQ3FNeHGi0jIuG+CC24S3JaMoRIcZxYihG6EdN25xc0BM0RsbFBiWdybJwyWY+cWcXC5dsJp3JRl0kf7K2MmYz0ZQjQozjxFCMeSdqfIKfamWTtN5nZtsoAFb27jN20Dx1aSaYeC7s2XQkmnrieVGXqOYYx4mhGKEfNT7BT7USNtb7bOjbj4iQzqr6EXBVpi5zADVaCmddLF5LqbY5LOi8qen7rxUj9GNOrYSN9T4ZBShV9XvWE3WZA6hZUzgb5whPjNCPObUSNtb7JIWipl83Aq7KGDNBHWGcIzwxQj8kqmV7r5Wwsd6no20UCKzYts8IuDwmB1Ad0WjHUYaMqPwyPi60t7erNWvWRF2MwCxcsrloE29tSXDV7LbGsiOa6E5DvZBJw1PfK93LaIK+KiJrlVLtftc1/pOoEXW50RcEYyc11AvNupehiZb9QUTmisgWEekWkRscPp8vIptE5M8i8oSIvNvy2edF5MX8v8+HWfg40fD+wMZOamhkMml4/Ba498Lc/5l01CWqGr6avogkgTuBTqAPWC0ijymlNlkuWwe0K6XeFpF/BBYCl4vIKODbQDuggLX5774edkWipq43+nRMNzGzk5qcKoZQaaKVrI5552ygWym1DUBE7gc+DhSFvlLqKcv1K4HP5v/+MLBMKbUv/91lwFzgt5UXPV7U9UafToevsc+3n1BftGwrv3huG/1pxfM79rOyZy8PXtNhBH+dE9lkHvZKNsZ7YDqlGAvssLzuA2Z4XH8V8P88vjs2SAFjT4wbVxudDl9jO6lfUNrynr30p3NOCApYt2M/dyzb2lib501IZJHPYa9kbYpURsHtmctjsTLVkU7i8J6jy4+IfJacKafwxLS+KyJXA1cDjB8/XqNIMaIRloUxM92A/8b4rEmjeX7H/mJnUtB4m+fVIsaKSmQOEWGvZG2K1O71S1n8xtmxSOOh09J9wCmW1+OAXfaLROQi4CbgXKVUv+W759m++7T9u0qpe4B7IOeyqVGm+NAIG5wxDNf3C0qb3zmFlT17WZcX/A25eV4tYqyohB6MqDvBhbWSLdzvQB8kUpBNQ2oEXeqM2Hj36Qj91cCpItIG7ASuAD5tvUBEpgN3A3OVUnssHy0Fvici78y/vhi4seJSx4kYasmBiaGLm9/GeCqZ4MFrOrhj2dZYbJ7X1cZyjBWV0B0iaj3BWe8nKXjHiXDmZ9g+eAmty3dweDDL0S2KG1oegHtvjmSl5XsnpVRaRL5CToAngcVKqY0iciuwRin1GPAD4B3AQyIC8JJS6mNKqX0i8h1yEwfArYVN3YYhhlpyI6CzMR6nzfO6ysIZY0VFu011NfhaT3DW+6k0HDsOLrqZ6zJZkBRdPXu5oeUBZux5MFeeCFZaWtOLUur3wO9t791s+fsij+8uBhaXW8DYE0Mtua60zgahZrboMOzxjaCo6GrwtZ7gXO5XMpnde3Puc4hkpRWP3RtDqNSV1tkg1CwLZxjmimopKiFvEA9RXi6cSOqZ/y/3+wf69DT4Wk9wOveLeKVlhL6NRtCS6yolRIw9SYJQ9eC8wnNadXds7fFh28/tysucl37KzFcfzNvLkyUbpa6CM+QJzlc+6Nzv/Jtyqcufz4crqUyufWvU7+tvdFWZRtCS6yr3e4w9SYJQ9f0F63Mq3jRe9viw7ed25eWde1YeMYuoDBx9Ys5mXkMTVSjyIZkCEeg/kHteq+7OTWI16vdG6NuoKy3ZhXK0zoaJhGxUrM8JoOVomHFNvOzxIZst7MrL6yfMhD0vHfn9Mz9TcwUhNPkQYb83Qt9GXWnJLpSjdcYtErIRzGyhYn1OkoThx+ASIxkdIdvP53dOIaHSTPjzj5iV2MiJEy6Ctmtg+x8i24AOTT5EaNc3Qt9GXSROq4IdPG6RkFWZhMJ+brXcjyg8p/X/G95+Dd58BVbeRazMYSHbz1PJBNe3PALp/5vTiv+4DWZ+Gb70RGj3CIqufPBVWiL0oGp6oe/UOAUtObbaZhXs4JGtcFwERVUmobCfWy33IwrPqfeZnMCHhjOHOY63mJn/dFfRvkpLhK7eTS/0vRontpu6VRgI8zunoLKKR9btBCCTVaQz2cgmuapMQmE/tygEUg3NArVWehzHW4wDybyI895g0wt9r8bRabhIVgNVGAipZAJJCG8cHuTwYJZfrthOMiGRTXJVMbOF/dyiEEg1NAvUWulxHG/X1GcgWZz3Bpte6Hs1jk7DRbIaqNLAj5N2UhUXyLCfWxR22RqaBWrdHxzHWwwj3nWI895g0wt9r8bRabhIBGU5A0Fj0zFy7aTaG6NhC5AAvxfb/SEPat0f4iwogxKnvFB2ml7oezWOTsNFLih10dh0jHzQNUiglhOx3R/yoNb9IY6Csh4naz+aXuhr4aGBRi4oddHYdIx80MXFU6ORXGIrIPL+EAO0J+s6SicSz1LFDQ8NtG4GhtOmY9w6alw8NRrJJdZQEdqTdR2tUo3Q1yEuGmglOG06xq2jVmFjNNDyvIpJzaq9IqylGaIRTR5uaE/WXjIiZsqVEfo6aGqgsR4MTpuOcZvMwj6yrvdZVqvT+dWOi3lrUPxt6VVMalbtFWEt9wyi3J+o9RjTnqy9ZETMlCsj9HXQ1EDrbrMuLuaUsLEMsuls4B+z+7mNy/1t6fWQ1MyFWu4ZRLk/UfUxZtPKU+ffpDdZe8mImClXRujroKmBLu/Zy+DgIAtSDzFLNtLz/Aeg867Ybug0xAlKTlgGWSv9zE5u5LYMJBNCx8RR7t+zT4IzromtXdZOWHsGOpp0lPsTVZ9w7Fq5Urk0yBoHq6fP/xaL0pexfMteZqW7jzy7mClXMZVGFRCh/WzWpNF8ZPfd/L0s4SgZ4IzDffDUKfEVHHUa+OKLZZANynCWZ0/Pf6CQbBoev8W5f8RsEgxiyghrz0BHk47SYy30CccuL3qfKdXKn//tkbz3PqYZ12cXs37VeEI/QvvZ/M4p7NnQzVFvDQDQovojX8oBsdtIqjqWQfa71yex6NDfApDJwsQXfnwka6O9f8RsEgxiyghrz0BHk47SYy30CccuL058b04bL2jl4GmasU7Mu/Yfcn52MetXjTfyozycIJng5OkfhpXbYrOUA2K3kVR1LIOsd8lmWrp6yeQ1w1mJjbGyr3oRhe3cSZOOk4NC6BOOXV5kM7n0zQWtXGVy3lwu43nRsq38qqubL6sHmJXYyMqWM7htcB5Ikl37D7FwyeZ4OXTQiEI/avtZrZdyOlp8DSbCOAkGK3bN8MTkxbCqdFKOa9mjsJ07adJ156AQBKu8SLZCIpkfu3OOjF1Juo7n5T17+bJ6gCuTOZPuVHYwvCXJ9w5/kj0H+1nc1Ru759V4Qj9q+1mtl3I6WnwNJsLYCAY/74vMt0ASZHufZpV6Lz/YfB5sWcGmvHDVKXutJokobOdOmnQ9RhNrY5UXkoBXNuTGiXUseYznWZNGM3vPJo6SnEl3BAPMTmwknZ0HxPN5NZ7Qr5XQjYudXEeLr8FEGBvB4DcJ5vvHbUsuy09SBxGOHDyoU/ZaTXBxifZu6Ghiq7y498IjB69rrojnd05hzUuzObxrB630o1Ij2HdCB639idg+r8YT+mGTScOT34Hn78+9fv8VcMF/i4+d3K7FT/iQs3dKCGXz0nBjIxg0TVnWSUpBUfDrlD02E1yNCHPFEVdTGlDWijiVTDDzqkXw1BjofYbshDk8O3AJI1/dzchWuPSssbHLx2WEvh9PfReW/yS3oQOw4n/mbXwxCbiwa/EqA6t+VpXJyEvDjU3iOc2Ba52khqeE008+lqxCq+yxmeBqhOeKI+CKNzZmQCfKXRFblKrbl2zmvpW9xb6REInPpJbHCH0/ep89IvABsukjGz1xCLiwavGZNNxxetUmIy8NNy6mCN2Ba5+kvnrBZH78ZDddPXtRbC3RQO3a6dcumByPCS4C7M/i+uT9JP6op2SkM1keXttXlVVSKCuIEFbE9bAKNELfj7Y5sGvdEcGfSFmESXwCLoCcxvXWq0deF8oaEnWh4WoOXPsktXDJ5sBnJcdtMNcC+7P47Mj/5GRNJWPRsq3sfbO/+DqVkND6kN8KolZmpXoYI1q1FpG5IrJFRLpF5AaHz+eIyJ9EJC0i82yfZURkff7fY2EVvGacfxPM+hq848Tcv46v5gR8Qbh86Ync/5YlbTqTZeGSzXzizi4WLtlMOpOtTVntq5KjxoQ6GX3tgslMO2kkRw1LMu2kkXz1gsmlF2Ty0a73Xpj7P5PW/u3InlkeJw1N57Nmw/4slmdPPxLE5LPiXd6zl4w68nrU0cNCWyX5tVFhUli/Yz+Lu3q5Y9nWUO5rZ37nFK7smMAJxwxnZGsLmayqeV/2w1fTF5EkcCfQCfQBq0XkMaXUJstlLwH/AFzv8BOHlFJnhlDWaEimoPO/5/5pEpnd0m5yOvMzoXoU/fjJbja9nNNiNr38Bj95sru0XhVsbpf7zMLS4Co9K7lZsD+L7e/7OrScorXitX/3kx8YV7G2XWj/XfsPkUwImaxybKNamV1SyQSSEN44PMjhwSy/XLGdZELis2+BnnnnbKBbKbUNQETuBz4OFIW+Ump7/rN4TWkRUa0O5ivgquGamd+oy/Y+w/G7xjM4eAmQdK6XbXM72/s0ty25TEsguz0zvzqHNcE6bkTn6379jmeYc8oZLByYR8fkE5rKhm/H/pz+qXMKJM8o67thPEdr+ycTcMIxw/nkB8YN+e1aTtxxt+vrCP2xwA7L6z5gRoB7tIrIGiANfF8p9Tv7BSJyNXA1wPjx4wP8dDypVgfzFXA2e3Y6k2XRks2VacF57T0xeIgr1AbeSma5LXO5sz3WttJYpd6rLZDdnplfncMaYI4b0Y9/p1j3malNPDpzTHXcch08YNIkWLRsK13dr5EUIaMUsyePCd0WHXSlVMmGfTU2+63tn8nCyceNcOxjtfQui/vKUEfoi8N7yuE9N8YrpXaJyETgSRHZoJTqKfkxpe4B7gFob28P8tuxpFodLKiAC0ULtmjvI2SA2ckXuC1zubM91rbS+MHm8zg8eFCrvG7PzK/OVR1gOm65YQTpOZjFFqUvK7ZdgS27D5a0oa7A9rqukj4SB5973favpXdZbNyXXdDpnX3AKZbX44BdujdQSu3K/79NRJ4GpgM9nl+qErXqpNXqYEEFXChasEV7P6SG0ZU5w90ea1tpzExvZtPut7TK6/bMZk0azV9ePkB/WiHkPA/SmWzx3tYB1tE2ikxW8Yk7u8JpXx233DCC9Bwml+UDF5YIfBjaho4Cu3PykElo0bJuV8Gu00cK42Zl9x4WDHuYGfICibZzub3/Eu557iUyCjb07UdlFd/86GnB6l4hcRSwsXFfdkFH6K8GThWRNmAncAXwaZ0fF5F3Am8rpfpFZAwwG1hYbmErJdaBIRoE7eChZEzMa+/Z3qdZr97LkwOXcpWmXTuMATm/cwore/ayrm8/SsGmV97gjmVbi+1mHWALl2zmvjDbV2ePJIwgPYfJZVb6SNsVsE+cjgI79cCQSWh5z4Ul1z20to+ufPvPbBvlq0gUxs1X1W84M7mEhAzA7k0cn91ORuWc9TIKHlm3MxSh7xQX8eMnux37bNwFbBzxFfpKqbSIfAVYCiSBxUqpjSJyK7BGKfWYiHwQ+DfgncDficgtSqnTgdOAu/MbvAlyNv1NLreqOlFusISxygjSwdOZLNmsYmRrS0k4eOCJL6+9J7iZDuDRKpXX6zey5A4wAu92C719dXz+wwjSc5hc5pNAgOe6Xyt6pZwzeUzJxOm48nOYhGZNuqx4XVJg31sD7DnYz+ZX3uDKjglcNbvNc2IuPNdZwzYyIp9YjPQhzmYDcMRD++DhwVBWWfY+urJnb6CEeAZvtIyPSqnfA7+3vXez5e/V5Mw+9u8tB95bYRm1cRKsQPG9BDltKYoNlrBXGTpeLb9csX1IOHjcPQucsKdMSICjcIlkAy0MjymHySWFfwCY40rqqaGT0Pzzj1y3a/8h9hzMBUgdHsyyoncfv7t2tud9Cs91efZ0psqOnOBPjeCN42eR+quQzuZMb4MZxfod+yvu3/Y+unn3wVD7rJu5quEPF8rTUDV0EqwKiu855ViplZ0/bGFbrldL3D0LnLAKtwS4an2R2HcjPBXJcSXlMAnZTWCFfqPb/vM7p5BQaU58fhjpzEjU8BTy/k9x9nn/wn99YpvjZFJJ/7b30anvOqbY5mH0WTdzVdWTJsYkM29DCX23qLzCe/1pRVbB766dXfyOV/h9mIQtbMv1aonjxpcfVqH1iTu74p//J0p8JqFy2j+VTHB9yyOQ+Y+c6UiNAEmQahlW0WSiW8avXjCZn+TzIoXRZ93MVVVPmhiTzLwNJfSdBJ0CT2FbK3NH2MLWbxIp3O+5vJ/3c92vofJHt9WzYKzHlUoUuK1gy86Y6bNhHWb/dipjmH3WzVxVaZ4qX6tBTDLzNpTQd+t4Xp2xakLE7wSnCvEbZIWBoywamN3Pux6px5WKLmGaGsvaQ/LSRH02rOtplVXoQ090f4mzho1ihmwg0XZexRHs9meusgpJyJGMpBM+RCIGmXlFqXjFQrW3t6s1a9bU7H7pTJY7lm0tESKh2PQfv+XIAEqNyB22XKWlnJew+MSdXazfsb947ZmnHFdi3qpXdARkHIKHgmA3kVw1u63sCbqsdr/3QthpGXtj23MJBSGvxHyvdMM6gD263tqiHOzP/IRjhhdz8LS2JPjirFO4vuXRsp+hHyKyVinV7nddQ2n65VA1DaWGSzkvra5RzSE6mmw9xGVYheGu/YdCMzWW1e5e2nyFG9ZRtEWtJxr7MwdK2vO5bQe4/tpoNv2tNJTQj0qbcLxvDQ9Z8dqXaFRziM5ejOs1MfGiAFvCMMnlmE+7ZIrUodAXu7pfY9pJIx39+12p4lnKUbgK13qisY+1TFaVuE3HReFqKKEflWbnHA5/JJJ1lXovP9h8HjPTm6syEXlpdbWwtUYx2eposo7XZNJw31zYuRZUNtrzjbElDFNwwjuGcfJxI4peKwsDJsyz9kUnE5FnW1XR/dTeFh1towLXLShhTzR+/dw+1tKZLMmEOGZujVLhaCihH1XgkeN9506Fi27mtiWFxFkH2fjKm6zs2UsWQu3oUWvzUUy2OnV2Dl76DvStoZgzMMrzjRkaeDbuuBG5CGTgh4+/WNQUdZ+rvS8WvLaW9+xlZtsoVvXuZX3fAZRC6zfDmtCdtOBQU2Y4ELZpM2g/T5FlQeoBFgx7FlJzgJti4bbZUEI/SCOHqZ163dc6CPvTqphDJsyOXkvPCafnFsVk61VnexkfvqbjSNv2PktpklipmReF07PzCjwb2dri+Fy9+q69LyZFioJqQ9/+kpOrdNoqrAnd3l5e8RZhoasM6coCr37u+BtOAj4GbpsNJfSDaLxhaqde97UOQkEvh0yccXpu5WhU1TQJebZt2xx45QXIHAYExn2wZucbu5616xJ4NrLVOW2IV/3sffEPL75aYj6yIuDaVoX2+eXy7VURzlp9xsUUott3dJUhXVngVWbH39jhIOBruNfnRkMJ/SAab1f3ayWd+cE1O8p21/S6r5smF8XGThiC1knbefiaDtdJz+2e1TQJea48nDYrq2hTDeKdYxcql04fS8JuE/apn70vruhxPs9XgOnjj3NVjKztUyCZEDomjhpSL3tf0ulnWgqaiykk7L7j9Dyv81mV2cvs2CbvKRXwmQlzWDR4KROG7WT28Bd415kfJlkjhcNKQwn9ICSl9GyY194cKEnZGxbWQegUE1BLCoNlcHCQj+y+mz0bujl5+ocDbSY5aTtOk15h4D+8to99bw2QzqqSAVpNk5B/SunLmP+Fb9XEsyuId46TUEklE45nDOiurDK2OJwTjhleslHsJpyt7XMEVbSMeQleHaGspaC5mELC7jtOz9NvVabzG0UFY9vTkEiye/0SRh7s4YaBebS0/BeuyrSxwOTeKZ+gWqx9MCjw7DzW35898Vj+OfUwie1/CLQDH3XUYmGwLEg9xN/LEo56awBWbiPIZpKuCc1JU7QO0GrGDziVsdabzelMltuWbuHnz/WSzub6mt07xy2K2q9/BDFjzp48hi35LJWFw28WzJ1KOpPlsp+tcN1jsrZPgUwWVvTuA7wFb2hC2cUUEnbfcXqe8362IpD93rFNkoncuHpcwaq7OHnwEJ+TbgaTcNvg5ZGZdxtG6Acd1LMnj+GFnQeKds5UQjzdyKy//5Hdd5NJLSWRPVyzHfiyTDN/UHixAAAgAElEQVQ2m+jsiZey+ZU3mCUbOarMRFO6gqkw8JNkmJ96iFmJjazIns7/ev2zLFyyma9dMLlqHkdOZdQWRJk02Sf/B6+s/0+WZ0+n931f57qLp5U8a522WLRsK/f+YVuJHT2VkKLQdSM9OMDq+65n1J6V7DthJh+88jZSLcN86+eG2wSxaNlW1u3YX9zSdovveMiyUrMKWC/BG5pDhcUUlzONXELXnV3MbBvFlbMmsGLbvlD6jtPzDGy/91gFOB05envm8iGnwNWKhhH6QbWL+Z1TUFnFI+t2ArlDRpTC1Y3M+vtn8wIt2cO5H6rSDrx9MGQtgR7l5lKZPwOYfTndz5/FtEM7GM4AhxjGuuzpnG3vfBX6ExcGzVfV/VyZXMJRMsBU2UH2kHD705ezsmcvD17TwYK5tenw2oLoqe+SWXEXJ2cP8zfqRX61YoA75MaSZ62jYCzv2Ttk49TpXGF7O5/z1zuZvut+RsgAh3b2svo+oePqHw0pZqWbmct79tp9mIrPxPrbl0wfiwhDBKzXaiM0h4pkKr/Jrti9binHHOxhw8A8nt+xn+mnHMf9X5rBj5/sZt7PVviesBX0uQW237vWkJIVSz/D6cqegWLoKXC1omGEftAlXyqZ4JsfPa3keDcvNzLr7/+RMzhD+mhR/cVlpzUSMilCRilmTx6j1RGdsA8GN/c9T2w20eT2Z1nwpW/zA/VP/LxrgFmJF+jKnMFP/trJFy2dL53JsvoX85m+635a6Uft3ogEXM0UBs1Fa7ZwVDq3qrAerL5ux/6adni3QWwXAtfveKY4oY+QAWaoDdxie9Y6g37WpNElLpIFLd9pRWBt548mVxTT/Y6QAUbtWeFYn0rNVbMmjWbTrv0MZHKvTzhmOF+9YLLjb181u21I3h6v1UaQlYjvs8wrLicPHuLz8iLpJMX+8+l7V5W4uOqcsKX73LzqENi8ZFmxPPb6JO7Y97fu9a0BDSP0wwhQ8mpM6+8fnPgNEqlJsP3ZogeIkw17y+6DZR/1Zh8Mbu57nrjYRLu2HWB9+nLg8uKl9gO3L97ZRavkDsWQMlYzhUFD6m9g5UslB6uD/x6KnUo9j9wGsV0IzDnlDD4gL9Ci+ovltS/DdQa900rS6yhCyLXz6tQZTMr+Nafpq2HsPX6mo8mxUru5/ezhA4cH+cmT3VXfZLfj+ywdTCO3ZS7PpUy3najldsJW2PmNAssaS6Rz75LNtHT1konIgw8aSOiHsUnq1ZhDf/+Mku86eTt4dUQ/dN333Ehnstzh4h7mpIXaA8qOzkzjPcmXGCEDHGY4reX6E1sOVv+P1yfzw7yWo9PhrYPV68Qsv+8GCbhZODCPh2aNZuuq/+A/D0/ljsw8Ui8f4LKfrShGUuvsR5SsJAumssVD/c0TUDzhrbUlwavt/8z6nccyas8K9p3QwR/GftHR5FjpZqbT2cOF6N1d+w8Vz+WttmDyFaB200heaXA6UcvthC0nhazwG+XUrRJZE3X0PDSQ0A+DShrTydvBrSMWPDsezWuBl0wfy/Uffk+JUNJ133Nj0bKtLF6+g8ODH6fV5h7mqYVm0nyz5QFGJDeyMftuWhJZ3h77ITrK9Se2HKz+iUyWnqVbivfNZJXnRpZ1sBYEI+QE1ENr+zzjKsoNuOmYfALJzm/zja0XFdPkZlwiqbX7iYu/+W1Lt7Despk67cSRzJ97OqnkERv+v7qYHKuxsrVG7yYTOZPPJz8wrqqCyXfMWUwjqQlzOJy+lDO3HXA8UcvthC0nhUyAaSeNrLnQjdqDD4zQD4ybBllyUlVeSzpn8hjHjmj37Pj5c70kE1IilCrtHH4BPPb9jCJPfZeZex5E5BCHk8NZd/Kn+OBVd+TczyoklUwgCSnmGP/liu1D6u1WB/upD/ve8o6r8MpB49RudkERaiS1i7/5o+t2YhVFffsPDZnErOU4ukVxQ8sDcO/NpNrmsKDzJm07/uGBNJ++dxWbdx9k6ruO4TdfmjGk7s9ZAhYzWXizPz3kudcci2kkCVyf/1dA54QtJ4VMAVlFw+X018EI/Ty65gA3DdJLSDu5D1o9O9JZ5Z/HI2DnLHv53/tszoYPtNJPh7wQisAvECR/ycy2USXJyJKJBG/ndx7tz8yOlxar026hRlJXEHpvLccNLQ8wY8+Dud8J6Cr86XtX8af8yuVPO/bz6Z+v4tEvzy6pu1qyuejTD/D2QIbFXb2xPIfAD+tqWinFaSeOpO/1t3n97cGKUlc3Akbo57EK8w19+3l4bR/zPjBuiMANY5PLy6buFzSjS9nLfx8BVemEFMT/+cqOCVw1u62s/OReWqzbJp/bWbIVR1K75Km/ZPrYYvBWKiFcetbYIV8tmZTuvTnXLhDYVXjz7oOlr185OOSawjO7b/n24uQapI9HdjqWg3vxomXdJavp198e5EvntGntiwWpR6V1juKZNZ3Qd3vI9rzmew72O2o5YUQDetnUnYJm7lu+HZX/nm6HKNs85HOQxqJlW/nFc9voTyue37G/6G+vm3cliP/zit59/O7a2cU8KCu27dU+GMRef6sWq5u8zKk+ZeVncslTf/2H3zM037oXFawYpr7rmKKmDzD1xGOGXFM8VxlKcvJb+7hX+0Z1OpaTe/HynguHrKYL/ck+JiqJiam0zlE8s6YT+m4P2cnu56TlhLGB5mVTtwfNQI2X2T4HaSzv2Ut/OldCRc5UcNnPVhQFv2cnzqRJPfVdFux4lgXvmUP63BtdzTlugrncs2Pd2s1t5eaXOyiMwzACT8wOE7KbELa//+svfJDP3beaza8cZOqJx/CbL84I/KzAW0jZn6XfZnslWNvnnv4uWhOl7sWzJl3m6aFmxb7KF5Fi6gy/lU6lK/9I0pJX+fdjh9tD9gs7L1Dt3fdZk0bzl5cPFAVrAb8OUe4yMej3Zk0azfOWlQhQEmjl2YltXiyrt+1j8Y65ruYcnaySuqSSCa7rnIIiZ6ZRbGV+5xTXE52swr5Ayb0rPAyjnPZKk2BR+jKWD1zIrPRo5uM+yTq9/+iXfQ5Gtzwrtz7u1RZ2s+W+twa4fekWJCGhmi/sJtCu1DTeI6XuxfPP14uTsNcpoziyaw8kBXbtP8TCJaWn3hXaz8+91S8bqd1ltxb7DE0n9N3MM4WOfl3nFD37rYampzOw7ddYfcCDbCCWu0z0+p5bYqmVPXtLTAXWQCtP85fNi2XUnhUcHrwYKDXnVJJVMmhdvU50sjM8JSTIRW7/9O2lnFzBYRjltJfTd5y0666QgpCc8AtgfHhtH3sO5rTudF7oFry1wjJf2E2gi9KfRCnhnNRGDo09h47z/8XbQ82jTlZSCQBxNPWWZE/1cG/1y0a68eUDKGrrQqol9EVkLvAjcl5TP1dKfd/2+Rzgh8D7gCuUUg9bPvs88K38y/+hlPpVGAUvFz/zjLYmr6Hp2RtcZdUQrcd6zV9ePlBynOLXXPyOrVR62IWX5ubWYR+8piOnaeUHnnXwez5fm0163wkdtPb7Rxnbf7Ocs2Nd6zrX/USnAskEjD56OOOOG1GchP/vsMl8IdldkoojCPbzHJ7rfk07iZ21/FaBlZScdr3nYH8oh6w74RfAOO8D40pMcYWyWstsz1UfNFWJ3QSaIcmvj/ochwpCN+BKwm2VP7K1pTiB2cdGyeogCycfN8JxMvPLRmo1ldbKhdRX6ItIErgT6AT6gNUi8phSapPlspeAf6DUhRYRGQV8G2gnV6+1+e++Hk7xg1ORecaq3R/o8z32zN7g9z63DaVyS8jCJmhGqeI1/WlV1KCtE8DMtlFksopL71pektfHPmkUCDLIZ7aNYsPOA2SyquSQDKfyFzpsKpngwWs6XFdEhQGZySpuW7qFlb37coP5whsRBbvXL6VLncG2sV/kylNSQ5J5Oa0wrKaZFT3lpbbQWTGUCNG8sC9ocNZ0uwsH5tFyVJLZyY3sO6GDD557A7jkX3IyC2y1edMkExK4/B1to8hmFSNbWxjZmrumIKT80jiXi9/4cVo52T2u7IrOI2v72PNmv7a3mtUEKuQOg3nwv3YEEphufczap728xXRXn2FlIw0THU3/bKBbKbUNQETuBz4OFIW+Ump7/jP7mvjDwDKl1L7858uAucBvKy65B35mlbLdpKzavSQhkYJs2lXTsy8b05ano8jZwqefclwxp44V63m69s0lyOX1sS/vAY4aluTKWRMCDnJ15H+LCuXVKXVy2VjLXRjMistZ/MbZud9c2TckmZeby6rVo8QenesWeGVHJ+maU9pep5w7SJLvHf4k6ew8WvsTXPXEtpIy2tvJySxgJZNV+OEnUKedOLJoSrHnzq+VW6C9X6Qz2SEeStbJsz+t2J2fqEA/Q65TtHoQdNIjO5Xdqwy6ZQ36G2GjI/THAjssr/sA961//+8OdUYOGT97adluUlabtMrA0SfCseMcXRsBvnbB5JKkVnYUucNcrprdVuIbXfw8/x375hI4L+9bWxJcOWtCIJvpyt59ZPLyx3pIBnh3Sl3X10K5C+Ut/G19zzrAFy3bWrJfUBDqIlL8XsEGWjAtuQVe2dFNuuaUVbLwPAqbg9ZNXqe6WcvvZhYo0NqSYNbE0b4mK3v57VlhC33JKXd+rd0Cwd3ddUi0s+U71hTPbuis1v0mOh3ngDAyiYaVjTRMdIS+07pTNzpb67sicjVwNcD48eM1f9odvwbVDdEfgt1P+szPlNjxnfx9N73yhqPAh9xgL/ibr7Bsjgq5jaEDea0tKQzR9Aua93XW9A8ixbroanPlaPOg5/pqLXfhtxV4LmmX9ww90zWZEGZOPPK7w1PC6ScfS1bhGXili653kDWNhFNbWOtWwLr5a3dLtZqPrBvIuoLZ3nbnTB7jmju/nOdT6QrBrY/Yo5032kw15Wi8QX3tozKtxAEdod8HnGJ5PQ7Ypfn7fcB5tu8+bb9IKXUPcA9Ae3t7Rek+dNygdEP0h6ARuOSWAx9yppfPz3w3SOmhFIVdfDhyYPVvvjijuInb0TYKJDd4rXl9iknY5k5FLdlcvLfdpOBFuUtMP9dXa7ntNnuvScrJJTSTVUN+V5FbpSigY+Jox8ArtzMO7MIriADwM6eprOLhP/Xx1kCGdwxPMfa41pL9hys7JnBlx4SiK+El08f6Hs9nrYvvEX0O17rFP3j9tm/MhQZee0IL5k7lunyqhL79udXzpWeN5Z8vfk9Zpqeg50/EIdtlVOgI/dXAqSLSBuwErgA+rfn7S4Hvicg7868vBm4MXMoA6LhB6YboD0EjcMn6O/Yc+G6mF6dd/NZhKc+lnz1TZ+Gehf91A2PKXWJ6ub4WNl0LQuTha0o32bwmqWKed4tn0DmTx5SUc+GSzSVasZt/v9sZB3bhFUQA+JnTCiuB/rTi0ECGN/vTJe2yIr+pbU865zfxBDmizy+dha5bYZAVsdPE4VQnt3TZrS0JEiJl7zUEPX+iEtNKmHsksUzDoJRKi8hXyAnwJLBYKbVRRG4F1iilHhORDwL/BrwT+DsRuUUpdbpSap+IfIfcxAFwa2FTt1rouEHphugHxdrJh6eEscfmXCpGtpYGhwTVxJywZ+oUjqxuwD8LZaWdrdyozcK9H17bV5ZnELina/AyaxRwmtSDCACnetsP6bD2v7cHMkNcJ7scNOCHr+koWclksqpoEprfOcXbRGOLGVnZfb7W83F7nm4xF14rYp0YCPsB9fYN+UpiCewePWOPbeXSs8b6nqPrFCfj5j7qFKXtladLh9imYVBK/R74ve29my1/ryZnunH67mJgcQVlDITdprzz9be55M4u16U9hLfU88rMiML1QBA/TcwJe6ZOxRG/bPDPQllpZ7MnJLMOHLsPutNm7d43j3hs2EPk/YRwOe5yBaxRt+VMeE5lW2hZtSQddrFGHX3EdfKrF0xmRc/eIeZHr5WM+NXZFjOy4IR9XNkyV0+JKJyfMLyLrsw0fiqXu8ZceK2IdWIg7NfZN+QrsatbgwYVsL7vACIyZJVpxz4OvE66c1o5euXp0sGkYQgBe6DFq2/m/oHz0h7C20W32ipnff/Jksa0RibaNRw/TcwJp0yd7xt7rHYEbxidrWBi+tWK7RzK/9bmV95g2okjPZfW9gnL6cBwL4K6y9nPOChn09QLu8fSiHzdC8Ks4DoJOYHuZ350ahvrSmBInW2RzjNkA1fN/ketoL6Jz9/Ox/r/D8Okn6mpHZx18ig+2PlRINiK2L7KtW5eWydV63UJoHVYkncMT3HpWWPLDrorlNW6pivkhbp96Ra++dHTXA8usj9rr5PunFaOBcodQ1FsKDec0C901Oe6XysGqhQoNIw9ItCtc5VrAnHSZAv3h1INxy23hx9OmTq/fuGpvhG8Bfw6m07d7SamQh3dXAfd7u10YLgXTpO0W3mdJnO7q2M5WUy96vMPHRNcU/jqmB+d2sZTMbF5lSXazmPBRd5KTEFr/a2sZVg+WZnf+Qm6PudeR1valbK3BzJklSIhwo+f7K74sPf1FndfgEfW7eSbHz3N9eAiu2VAKTUkl45Tnp1kAoYlEwxmVEVRz1FsKDec0C+QlKHrbKeIQK/OVa4JxEmTvWT62KILWcHlsO/1Q8Wwed3loV24Lb/hgiGbpIVJbd7PVrgKbPukYT+6UKfu9npCbjJzcx203jvsju6X8tmK3exTaRZTt2Ahp3bQ0ewCPx8frzInClrr8tTpTJUdjJAB33QSuj7n9knVqgEXruvq2VuS4uChtX2cdGxrRavP+Z1T+KVDvEuhvk4HFxVWUIVJ6NDg0Fw6bnl23I5n9MNJQVkwt3YneDWs0M/YnONHtCT5wuwJfPWCycz5wdNanatcE4iTJntd5xTHyES33B5Wgh4QriOwrT7nVi+SIGly7SYm0POzrkZQil2Dtmb+tFMQqou7tnNo8MhhITr5b5xw299waisdgR74+fh4lTlR6KOLBj9JMpHgY+94kZOnzy1OGH7ZIb3OeNZNd2HPyDnuuBGeZkE/UskEn5v5bseDaZzModYVlHUSymThpGNbUcC8n60oSV5nz7NTTj+OKliuQMMK/dmTx5TYH78wO+dat3DJ5iGmF7eNvXJNIG6an72D6Nrzgno86E5WQdPk2oWok4mpXD/rSrH791szf9optIU1IA708t/44dtWDpubflTDrc/aRw9OupETOksTlfllh/Q641lnYnPKyOlnFrQ/i5ltuTxRxdxOnVNcD6bxOrgIfA6Jl3CT10WxeWulYYW+16EZdtOLAseNPT/3PDetW1dT013GB/V48POP1jE1OA1KJ1dH3fS1TvcHQhNmdv9+r83EAvbVoE7+Gz/C8k4JuroLil8f9csO6XbGs+5JY04ZOb3Mgm7ukvbcTm6xC3591ctTySl5XSUTse74rFp+pKr8agxw69ROphcnv+kFLr9hdc+r1M9YN4eINcLYnoLATYvy8o/WMTU4DcpKNByn+1sTlOkKMy+/6plto5gxcRQrtu3TEpT21eA5k8eUXb8Cdi8Wv7ZyqpM9e6q9n2mnDQmpHk6eOm6nUjm1c6H/ua2IdVKIuLlL2nM7lasx+3kqWT2woFQOBJ2IncbdbUu3FM1SG3YeQCnFNz/ir0yVQ8MKfTecHrhiq5aZxR5UFKafsRv2COPTTz7WN42s02Si60dtxW8lEkQ7cbp/4e+SMmk8Dy+/6kKyNK/NRN36lUM5GSD9DkfxSjBnP4NBZwLQaTc/Tx03U4lTOyucTUVBUog4uUs65XYKC7++UYmJxml8PrpuZzHGJpNVPPKnnUbol4OuG5+XGceatyUpUrIfkBR4/ynH+WpylRDWQQs6+wfOXgXuk0KQDSmn+/slX3NC169ap77V2FD2+k2dzKSFOnitGKymB2sK7mIbdE72PNVNd6Pfy1PHaiqxm6Lsm7FuK+kCOgLU6ZyDS6ePdcztFAZhBQi6Ye8Lyi0rYxVoGKHvNKBuW7qluOG0oW8/KqscbXp+ZpwC1mU25LSMsyeMKvFcCLse5aRocEJHqw3qVWAdrIODg0x4/jZ2bdjE8uzp9L7v61x38bTic3G7v9fyXmcfYuq7jnEMSAtLiw/z7GGvzKTWFAIJKDk2075isJoehCNZt4sCM/WA56lu5WipXs/BWi/rBFVIJ1Hi3+4Ql+JUf6v7MISTQz9MKu1f9r5w2ruOYd/bA2Syuf30S6dXLwN9wwh9pwH16LqdRbtjRsFdz27jVyv/ymdnjOcb+eWlG07LSesyG3IbWHZXx7DrYU/REDRq0T5YvcLSgwoDqwD+xrCH+btDS2iln79RL/KrFQPcITcWn4ub5uS1vNfZhyj4SjtNHBVr8Zk0q38xn4t3dnF0Zho/feVyxyMvnZ6nn8nG+nyLm9B5jX3TK2/wkye7Xcvvle5j9qTRQyJ07ae6lbOR6KUQWOvVn1ZkFfzu2tkl6SSSiVyk8mBGDYlLmd85hRXdr7GuL2fGfL5vPz9YsplkfhWks+qsNkFXwX7Y+0IWuGbOpJoEaTWM0HezGdt5eyDDvX/opSWfDdKtozvlbSloMZtfOVgMAAnb5cotmVgh0GfOD54uei/oaOOVmmC8sAqfv3u7m9a3cqavETLADLWBWzSfi1tMgO4+hH3ieGHngYqSYBV56rtM33U/rdLPe5IvodLCr9d9rhjb4GVP9zPZ2DOTZnHQ2F2KZY8LuH3plpIgu+yED5GwnvtgC7jS3ei39hM/916nepWkp8iCpIR01tlhYueBw8V7ZRT8euVfUfnrovBltxO2b73uWQjVoGGEvlPuD6XUEJMMHPHhdttgAve8LfYBUonJJYgbpZP3QkFAdrloZ6CnvVv3L6adNLKkrl6UaO+Pf5jBrtxB4YfUMFbxXu3n4hYTUG6e+0qTYBXpfZZWjkxks5MvcPvB/mJ/crSn5+/lVPbrPEwC5dqInYLsUrPmcf3MhGuErvZGv+Vzt/KlM9mSc3qtm7q6pjgnCukN3MpTa8L2rY8yn3/DCH23JW8yAShK/IoFfDeYvDZywmowqyAvaI0ZpRwFr5v3QiGNg5v2oSNM7JPYVbPbggvK828ioWBX/tDzN9/7Nc+Dzu32WqeYgCFJxi6cCI/f4rhB6bQyq3hwts1B7d6IpA9xiGF0Zc4YqkC4aOe6AXp4XK/LkLz32w5w/bVDI3S92sGvn7iVb9GyrSXn9Fpz4ruZ4pzqeMn0sSWRtO8/+Wgu3vMLzuYF/sgZHJz4De3n4YdfRLETYSdGq4YTgfa9I7hnVXDL/ZHJwvvGjiSZEDbsfINhqQR/P2O8r6umTp7tSvNl2G2hhehQJ8Hr5L0A+KZx0BEmoWgxyRTJzm9zcue3+aTtI7+lsVtMwJCB8fgtrhuU9kReobjxnX8Tks9p8++vT+KOfX9b8vGIlkTRBBHkqEknKhEC9qRhbgn8vNrBr5+4lc+r7zh9x62OQyJp5beofUtpyR7mDOkjkZoEnFHG0xmKX0SxE1Fq5mHTMELfin1W/tCpx3vmYHFqyCB5tsMop5XDg1kWd/XyXPdrxTMA/OywlbglVju9q86kojWoPDYoi2mtO6d4HsASCEtOm94lm+GZnqKtMJUQz2yatcQ+4bmZtoIKaB3C6jtD7n/vc5DN2flbVD9sf7as37VTiLVxiyguXKObsbUeaUih75c+Qachg+TZrrSc9zlkBjw0mOX5vgMl3iw68QV+BDlrNSxC85m3H0yf36CsRdZCrzxDOv2gmmH2bpkrde3yleDWdyqur0tbV4o97TkMPcRHd9NW9wzmuNGQQt/P715HU9fZhKq0YxcjEqGYFtiO1wRTjvbh1qG9fscv42LQ6M6yD8twSSFci6yFbrlbdPtALcpYrl0+SD3suPVBrxgZrXuVkS5aB3veoFRC+NKH2oaceaCj4Dk5V7hFFMeJhhT6TgS1W+tsQoU1kJ3s0QWcbLRBB6hVI3lxz5uBVyx+GReDRneWnbfEJYVwlFkLrfX3Oi+1FmUs1y5vr0cYk5I9RqZwmIn2vcpIF62DfWJ0clrQXRE5OVfEwdPIj6YR+kGXtjqbUPaBXO4JTHZ7dMFNdMe+t3n97cEhNtqgA9RJI4Ghy1o3/DIuBhVmYQvAau9JeKHrKlqLMlZidw6rL5dzLx17eiXY0zFfOWuCZ+oGXXOn055crftfOTSN0NdpyKAdLuwTmOyD9hN3dvHqmzmPHuvgCCo0nTQS0D+b1i/jYlBhFrYADGtvo9KcRgWc2iTu3h9h92W7C2bhMBP7veztXw0zmP03C0n53AiaGt0plsdOLVMn+9E0Ql+nIYN2OKeN2DCXd26DI6jQdNNIdM+m9cu4GFSYWb9TyM/ilffej7D2NrwitP3q4ucqGnfvj7D7stthJtZ7OX1WDTNYtUxrQdo06tOyrDSN0NchaOewbsSGlXfeitvgCCpow9BIvDIuBhVm1u9Y87PUcjAESQGsU5dQXUUjIOy+XG6f0Um+FpQozX8Foj4ty4oR+hZ0g1zsVGvp7jY4yknfG1eNJKrB4CQI/FIAexF3TV6XqM1QTsnn3M46DvKbZdUpk/ZMUR2EOEw8BYzQt6Ab5GInTgO+UqFdayEc1WCo5DCdRqZafVnXpu2UfK7Sk8LKrtNT3/VMUR2EqCdTK0boW9ANcokzlQrtWgvhqAaDXRB4JQ6rNXHa9AuLSrK9Wk8Kq6k93CdFdRDipBgaoe9AnJZiQam07LUWwnEZDF6Jw6IoS1w2/cIiiDJi74PWk8LCVMJ8J9cqRQVHjZbQF5G5wI+AJPBzpdT3bZ8PB34NfADYC1yulNouIhOAvwBb8peuVEpdE07Rq0eclmJBqbTscRHCtSZOG21B0mHXy2ogiDJi74P2Q8rDUsJ8J1dLVHBmwhwWDV5CVwVeZnHBV+iLSBK4E+gE+oDVIvKYUmqT5bKrgNeVUpNF5ArgX4HL85/1KKXODLncVaWeBV8tyq4jcBpZKMWhLDmx7+oAAAdKSURBVPWyGij3rAYr1VLCfCdXS1Tw7Us2s3h5/J+3Djqa/tlAt1JqG4CI3A98HLAK/Y8D/z3/98PA/xQRCbGchhjhJ3DSmSyX/WyF6wEjcSROq7uapcOuAWGc1VAtRabcQ3rCet5RJWzTEfpjgR2W133ADLdrlFJpETkAFJ5gm4isA94AvqWU+kNlRTZEjd8AWLRsK+t27C8eOBJnoVQgTqu7ctJhd7SNKi+JXZWJ8+QUZKKvxkowqoRtOkLfSWO3p4N0u+ZlYLxSaq+IfAD4nYicrpR6o+TLIlcDVwOMHz9eo0j1T72ZP6z4DYDlPXtLOkjhpDId6vm51LLsdoGVyapIgtz8iJPZzE6Qib4kwFGk6EZaSRtHlbBNR+j3AadYXo8Ddrlc0yciKeBYYJ9SSkHukFGl1FoR6QGmAGusX1ZK3QPcA9De3j40v3BIxEmg1ItN1gk/DckeVTl9/HHa5pIgzyVO7Qm1bVOnPE1x1KjjZDarhGLEsiVDbKVaeVQJ23SE/mrgVBFpA3YCVwCftl3zGPB5YAUwD3hSKaVE5Hhywj8jIhOBU4FtoZU+IHEStHFe9vrhpyG5nQ+rQ5DnEqf2hGjbNK4adZzMZmEQZhsHSY8SJr5CP2+j/wqwlJzL5mKl1EYRuRVYo5R6DPgF8L9EpBvYR25iAJgD3CoiaSADXKOU2leNiugQJ0Eb10EaBmGd9xrF5lolRNmmjaJRx50w2ziqCVHLT18p9Xvg97b3brb8fRiGnIeNUuoR4JEKyxgacRK0ZpA6E/XmWiVE2aaNplHHlUYYt6JU1UzoZdHe3q7WrFnjf2EZpDPZIZkQ62WT0DAU054GwxFEZK1Sqt33umYS+gaDwdCo6Ap9oxYZDAZDE2GEvsFgMDQRRugbDAZDE2GEvsFgMDQRRugbDAZDE2GEvsFgMDQRsXPZFJFXgb9W8BNjgNdCKk7UmLrEE1OXeNLsdXm3Uup4v4tiJ/QrRUTW6Piq1gOmLvHE1CWemLroYcw7BoPB0EQYoW8wGAxNRCMK/XuiLkCImLrEE1OXeGLqokHD2fQNBoPB4E4javoGg8FgcKFhhL6IzBWRLSLSLSI3RF0eP0RksYjsEZEXLO+NEpFlIvJi/v935t8XEflxvm5/FpGzoiv5UETkFBF5SkT+IiIbReTr+ffrrj4i0ioifxSR5/N1uSX/fpuIrMrX5QERGZZ/f3j+dXf+8wlRlt8JEUmKyDoR+ff867qsi4hsF5ENIrJeRNbk36u7PgYgIseJyMMisjk/bjpqVZeGEPoikgTuBD4CTAM+JSLToi2VL78E5treuwF4Qil1KvBE/jXk6nVq/t/VwF01KqMuaeCflVKnATOBa/PPvx7r0w9coJR6P3AmMFdEZgL/CtyRr8vrwFX5668CXldKTQbuyF8XN74O/MXyup7rcr5S6kyLO2M99jGAHwFLlFJTgfeTa5/a1EUpVff/gA5gqeX1jcCNUZdLo9wTgBcsr7cAJ+X/PgnYkv/7buBTTtfF8R/wf4DOeq8PcBTwJ2AGuUCZlL2/kTtGtCP/dyp/nURddksdxuUFyAXAvwNSx3XZDoyxvVd3fQwYCfTan22t6tIQmj4wFthhed2Xf6/eeJdS6mWA/P8n5N+vm/rlTQLTgVXUaX3y5pD1wB5gGdAD7FdKpfOXWMtbrEv+8wNAnA48/iHwDSCbfz2a+q2LAv5TRNaKyNX59+qxj00EXgXuy5vdfi4iR1OjujSK0BeH9xrJLaku6ici7yB3JvI/KaXe8LrU4b3Y1EcplVFKnUlOSz4bOM3psvz/sa2LiPwtsEcptdb6tsOlsa9LntlKqbPImTuuFZE5HtfGuS4p4CzgLqXUdOAtjphynAi1Lo0i9PuAUyyvxwG7IipLJewWkZMA8v/vyb8f+/qJSAs5gf+/lVKP5t+u2/oAKKX2A0+T26c4TkRS+Y+s5S3WJf/5scC+2pbUldnAx0RkO3A/ORPPD6nPuqCU2pX/fw/wb+Qm5HrsY31An1JqVf71w+QmgZrUpVGE/mrg1LxXwjDgCuCxiMtUDo8Bn8///XlytvHC+5/L7+LPBA4UloFxQEQE+AXwF6XUIstHdVcfETleRI7L/z0CuIjcJttTwLz8Zfa6FOo4D3hS5Q2vUaOUulEpNU4pNYHcmHhSKfUZ6rAuInK0iBxT+Bu4GHiBOuxjSqlXgB0i8p78WxcCm6hVXaLe1Ahxc+SjwFZy9teboi6PRnl/C7wMDJKbya8iZz99Angx//+o/LVCzjupB9gAtEddfltdziG33PwzsD7/76P1WB/gfcC6fF1eAG7Ovz8R+CPQDTwEDM+/35p/3Z3/fGLUdXCp13nAv9drXfJlfj7/b2NhjNdjH8uX70xgTb6f/Q54Z63qYiJyDQaDoYloFPOOwWAwGDQwQt9gMBiaCCP0DQaDoYkwQt9gMBiaCCP0DQaDoYkwQt9gMBiaCCP0DQaDoYkwQt9gMBiaiP8fWAGKkLOoe40AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot error\n",
    "groups=error_table.groupby('actual_class')\n",
    "fig,ax=plt.subplots()\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index,group.reconstruction_error,marker='o', ms=3.5, linestyle='',\n",
    "            label= name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](classes_scatter.png \"reconstruction error for all instances in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color on the plot identifies which data was indeed in the positive class. However, we aren't using that label in training, as this is unsupervised learning.\n",
    "\n",
    "By coloring the points with their appropriate class we have shown that the autoencoder is able to predict which aircraft are spyplanes significantly better than chance (guessing randomly would only identify 15 spyplanes correctly on average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the model to the unlabelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a good degree of confidence that this approach can distinguish between normal aircraft and spyplanes, it's time to apply it to the whole dataset and see which aircraft are identified as most likely to be spyplanes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](full_dataset_scatter.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the x value of a point, the more likely it is to represent a spyplane. Of course, we don't know how many spyplanes are in the dataset, so we can't pink a threshold value for the error. However, just for fun, I picked 0.2 as the threshold - it'd pretty arbritrary but above that we have only the most anomalous data points. So, I'm going to go ahead and say these points represent spyplanes. Let's get the identification numbers for those aircraft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get identifiers for predictes spyplanes\n",
    "error_table['adshex']=df_adshex\n",
    "positive_identifications=error_table[error_table['reconstruction_error']>=0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 160 candidate aircraft identified. Let's see how many of the ones I identified were in common with the 101 identified by Peter Aldhous, the original author of the Buzzfeed News article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare to previous results\n",
    "pa_results=pd.read_csv(\"pa_candidates.csv\")\n",
    "common=set.intersection(set(positive_identifications['adshex']),set(pa_results['adshex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 17 were in common with the previous results. This is not necessarily a bad thing since the two methods used completely different approaches. Both could be good classifiers, neither could be or one could be good and the other bad (or anything between these extremes). Without further data it is impossible to know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
